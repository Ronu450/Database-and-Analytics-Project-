

Setting up Pyspark
1.	Install Anaconda
2.	Install Java 8 and set the path and java home in environment variables
3.	Download Spark home using the link https://spark.apache.org/downloads.html and select spark  version 3.0.0  and Prebuilt Hadoop 2.7 as package type 
4.	Extract the folder and set the path for Spark home as this path and path as this path up to bin inside the folder in environment variables
5.	Download winutils for version 2.7.1 and create a folder winutils and a subfolder inside it as bin and put the file there. Provide the Hadoop home path as this path up to bin.
6.	Use pip install pyspark in anaconda prompt.
7.	Use pyspark command in anaconda prompt to check if it is successful.

Links for reference : https://www.datacamp.com/community/tutorials/installation-of-pyspark


1. run the command pip install -r requirements.txt  to install all dependent packages
2. Run the jupyter notebook 

